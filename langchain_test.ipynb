{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x7065d8cc0790> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7065d8cc3f10> openai_api_key='4b94ea21e11a4d46b7b5c17a1ef08de4' openai_proxy='' azure_endpoint='https://egn-llm-hackathon.openai.azure.com/' deployment_name='gpt-35-turbo' openai_api_version='2023-09-01-preview' openai_api_type='azure'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=\"4b94ea21e11a4d46b7b5c17a1ef08de4\", \n",
    "    api_version='2023-09-01-preview', \n",
    "    azure_endpoint='https://egn-llm-hackathon.openai.azure.com/', \n",
    "    deployment_name=\"gpt-35-turbo\"\n",
    "    )\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore programmer.\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = HumanMessage(\n",
    "    content=\"Translate this sentence from English to French. I love programming.\"\n",
    ")\n",
    "llm.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pprint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m res \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow can langsmith help with testing?\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpprint\u001b[49m(res)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pprint' is not defined"
     ]
    }
   ],
   "source": [
    "res = chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith can greatly assist with testing by providing a powerful and flexible language generation platform. Here are some ways in which Langsmith can help with testing:\n",
      "\n",
      "1. Test Case Generation: Langsmith can be used to automatically generate test cases for software applications. By leveraging its language generation capabilities, Langsmith can create a wide variety of test scenarios, covering different input values, edge cases, and boundary conditions. This can significantly increase test coverage and help uncover potential bugs or issues.\n",
      "\n",
      "2. Test Data Generation: Langsmith can generate realistic and diverse test data for use in testing. It can create synthetic data that simulates real-world conditions, ensuring that the software under test is exposed to a wide range of inputs. This can be particularly useful for stress testing, performance testing, and security testing.\n",
      "\n",
      "3. Test Script Automation: Langsmith can be integrated with testing frameworks and automation tools to automate the creation of test scripts. By leveraging its language generation capabilities, Langsmith can automatically generate test scripts based on predefined templates or specifications. This can save time and effort in writing and maintaining test scripts, especially for complex or repetitive scenarios.\n",
      "\n",
      "4. Test Documentation: Langsmith can assist in creating comprehensive and accurate test documentation. It can generate test plans, test cases, and test reports in various formats, such as HTML, PDF, or Markdown. By automating the documentation process, Langsmith ensures that the test documentation remains up-to-date and aligned with the actual test scenarios.\n",
      "\n",
      "5. Exploratory Testing Support: Langsmith can be a valuable tool for exploratory testing. Testers can interact with Langsmith to generate real-time test data or test cases on-the-fly, based on their exploration of the software. This allows for a more dynamic and interactive testing approach, helping testers uncover unexpected issues or behavior.\n",
      "\n",
      "Overall, Langsmith's language generation capabilities can enhance testing by providing automated test case generation, realistic test data, script automation, comprehensive documentation, and support for exploratory testing. It can help improve the efficiency, effectiveness, and coverage of testing activities, ultimately leading to higher software quality.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0x7065d8ce60d0> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7065d8cf8f50> model='text-embedding-ada-002' dimensions=None deployment=None openai_api_version='2023-09-01-preview' openai_api_base=None openai_api_type='azure' openai_proxy='' embedding_ctx_length=8191 openai_api_key='4b94ea21e11a4d46b7b5c17a1ef08de4' openai_organization=None allowed_special=set() disallowed_special='all' chunk_size=16 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None azure_endpoint='https://egn-llm-hackathon.openai.azure.com/' azure_ad_token=None azure_ad_token_provider=None validate_base_url=True\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "embeddings_model = AzureOpenAIEmbeddings(\n",
    "    openai_api_key=\"4b94ea21e11a4d46b7b5c17a1ef08de4\", \n",
    "    api_version='2023-09-01-preview', \n",
    "    azure_endpoint='https://egn-llm-hackathon.openai.azure.com/')\n",
    "print(embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader('queens_speeches/speeches', glob=\"**/*.txt\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    doc.metadata[\"year\"] = int(doc.metadata[\"source\"].split('/')[-1].split('.')[0])#.split('/')[-1].split['.'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'queens_speeches/speeches/2003.txt', 'year': 2003}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size = 100, chunk_overlap = 20, length_function = len, is_separator_regex = False)\n",
    "#documents = text_splitter.split_documents(docs)\n",
    "#input_documents = [text_splitter.create_documents([d.page_content]) for d in documents]\n",
    "\n",
    "text_splitter = SemanticChunker(embeddings_model)\n",
    "documents = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_document = documents[0]\n",
    "#test_document\n",
    "#documents = text_splitter.create_documents([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_documents = [y for x in input_documents for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = FAISS.from_documents(documents, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Besvar nedenstående spørgsmål, udelukkende baseret på denne kontekst:\n",
    "\n",
    "<kontekst>\n",
    "{context}\n",
    "</kontekst>\n",
    "\n",
    "Spørgsmål: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kronprins Frederiks første datter blev født i år.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Hvilket år blev Kronprins Frederiks første datter født?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
